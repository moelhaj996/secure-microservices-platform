prometheus:
  prometheusSpec:
    retention: 15d
    resources:
      requests:
        memory: 512Mi
        cpu: 500m
      limits:
        memory: 2Gi
        cpu: 1000m
    
    securityContext:
      fsGroup: 2000
      runAsNonRoot: true
      runAsUser: 1000
    
    storageSpec:
      volumeClaimTemplate:
        spec:
          accessModes: ["ReadWriteOnce"]
          resources:
            requests:
              storage: 50Gi

    additionalScrapeConfigs:
      - job_name: 'vault'
        metrics_path: '/v1/sys/metrics'
        params:
          format: ['prometheus']
        static_configs:
          - targets: ['vault.vault.svc:8200']
        
      - job_name: 'istio-mesh'
        kubernetes_sd_configs:
          - role: endpoints
            namespaces:
              names:
                - istio-system
        relabel_configs:
          - source_labels: [__meta_kubernetes_service_name]
            action: keep
            regex: istio-telemetry

grafana:
  enabled: true
  adminPassword: "change-me-in-production"
  
  securityContext:
    runAsUser: 472
    runAsGroup: 472
    fsGroup: 472
    
  resources:
    requests:
      memory: 256Mi
      cpu: 250m
    limits:
      memory: 512Mi
      cpu: 500m
      
  persistence:
    enabled: true
    size: 10Gi
    
  dashboardProviders:
    dashboardproviders.yaml:
      apiVersion: 1
      providers:
        - name: default
          orgId: 1
          folder: ''
          type: file
          disableDeletion: true
          editable: false
          options:
            path: /var/lib/grafana/dashboards/default
            
  dashboards:
    default:
      istio-mesh:
        url: https://raw.githubusercontent.com/istio/istio/master/manifests/addons/dashboards/istio-mesh-dashboard.json
      istio-service:
        url: https://raw.githubusercontent.com/istio/istio/master/manifests/addons/dashboards/istio-service-dashboard.json
      vault:
        url: https://raw.githubusercontent.com/hashicorp/vault-helm/main/dashboards/vault.json

alertmanager:
  enabled: true
  config:
    global:
      resolve_timeout: 5m
    route:
      group_by: ['job']
      group_wait: 30s
      group_interval: 5m
      repeat_interval: 12h
      receiver: 'null'
      routes:
        - match:
            alertname: Watchdog
          receiver: 'null'
    receivers:
      - name: 'null'
    
  securityContext:
    runAsUser: 1000
    runAsNonRoot: true
    fsGroup: 2000
    
  resources:
    requests:
      memory: 128Mi
      cpu: 100m
    limits:
      memory: 256Mi
      cpu: 200m

pushgateway:
  enabled: true
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi

nodeExporter:
  enabled: true
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi

kubeStateMetrics:
  enabled: true
  resources:
    requests:
      cpu: 100m
      memory: 128Mi
    limits:
      cpu: 200m
      memory: 256Mi

serverFiles:
  alerting_rules.yml:
    groups:
      - name: kubernetes
        rules:
          - alert: KubernetesPodCrashLooping
            expr: rate(kube_pod_container_status_restarts_total[15m]) * 60 * 5 > 0
            for: 15m
            labels:
              severity: warning
            annotations:
              description: Pod {{ $labels.namespace }}/{{ $labels.pod }} is crash looping
              summary: Pod is crash looping

          - alert: KubernetesHighCPUUsage
            expr: sum(rate(container_cpu_usage_seconds_total{container!=""}[5m])) by (pod, namespace) > 0.8
            for: 15m
            labels:
              severity: warning
            annotations:
              description: Pod {{ $labels.namespace }}/{{ $labels.pod }} is using high CPU
              summary: High CPU usage detected

          - alert: KubernetesHighMemoryUsage
            expr: sum(container_memory_usage_bytes{container!=""}) by (pod, namespace) / sum(container_spec_memory_limit_bytes{container!=""}) by (pod, namespace) * 100 > 85
            for: 15m
            labels:
              severity: warning
            annotations:
              description: Pod {{ $labels.namespace }}/{{ $labels.pod }} is using high memory
              summary: High memory usage detected 